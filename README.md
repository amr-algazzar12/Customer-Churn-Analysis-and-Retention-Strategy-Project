# ğŸ“Š Customer Churn Analysis - Modern Data Stack

> Predicting customer churn using **DBT**, **DuckDB**, and **Machine Learning**

[![Python](https://img.shields.io/badge/Python-3.12-blue)](https://www.python.org/)
[![DBT](https://img.shields.io/badge/DBT-1.8-orange)](https://www.getdbt.com/)
[![DuckDB](https://img.shields.io/badge/DuckDB-1.1-yellow)](https://duckdb.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

---

## ğŸ¯ Project Overview

This project demonstrates a **modern data engineering approach** to customer churn prediction, combining:
- **DBT** for SQL-based transformations
- **DuckDB** as an analytical database
- **Machine Learning** for churn prediction
- **Automated testing** for data quality

### Why This Approach?

| Traditional | This Project |
|-------------|--------------|
| Pandas scripts | SQL transformations |
| No testing | Automated data quality tests |
| Hard to reproduce | `dbt run` = Done |
| No documentation | Auto-generated docs |
| Manual pipeline | Version-controlled SQL |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROJECT WORKFLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ DATA INGESTION
   Excel File (Kaggle)
      â”‚
      â”œâ”€â†’ src/convert_excel_to_csv.py
      â”‚
      â–¼
   CSV Seed File
      â”‚
      â””â”€â†’ dbt seed â†’ DuckDB

2ï¸âƒ£ DBT PIPELINE (3 Layers)
   
   DuckDB Database
      â”‚
      â”œâ”€â†’ Layer 1: STAGING (stg_customers.sql)
      â”‚   â””â”€ Clean & rename columns
      â”‚
      â”œâ”€â†’ Layer 2: INTERMEDIATE (int_customer_features.sql)
      â”‚   â””â”€ Encode features, feature engineering
      â”‚
      â””â”€â†’ Layer 3: MARTS (fct_customer_churn.sql)
          â””â”€ Final ML-ready dataset (TABLE)

3ï¸âƒ£ MACHINE LEARNING
   
   Python Training Script
      â”‚
      â”œâ”€â†’ Load from DuckDB
      â”œâ”€â†’ Handle imbalance (SMOTE)
      â”œâ”€â†’ Scale features (StandardScaler)
      â”œâ”€â†’ Train 3 models (LR, RF, XGBoost)
      â”œâ”€â†’ Evaluate & compare
      â””â”€â†’ Save best model

4ï¸âƒ£ DEPLOYMENT
   
   Streamlit Dashboard
      â”‚
      â””â”€â†’ Real-time churn predictions
```

---

## ğŸ“ Project Structure

```
customer-churn-dbt/
â”‚
â”œâ”€â”€ ğŸ“‚ dbt_project/                    # DBT Project
â”‚   â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ staging/               # Layer 1: Data Cleaning
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql     # Clean & rename columns
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml            # Tests for staging
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ intermediate/          # Layer 2: Feature Engineering
â”‚   â”‚   â”‚   â”œâ”€â”€ int_customer_features.sql  # Encode features
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml            # Tests for features
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“‚ marts/                 # Layer 3: Final Dataset
â”‚   â”‚       â”œâ”€â”€ fct_customer_churn.sql     # ML-ready table
â”‚   â”‚       â””â”€â”€ schema.yml            # Tests for final data
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ seeds/                     # Raw Data
â”‚   â”‚   â”œâ”€â”€ Telco_customer_churn.csv  # Input data
â”‚   â”‚   â””â”€â”€ properties.yml            # Seed configurations
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ tests/                     # Custom tests (if any)
â”‚   â”œâ”€â”€ ğŸ“„ dbt_project.yml            # DBT configuration
â”‚   â””â”€â”€ ğŸ“„ profiles.yml               # Database connection
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ Telco_customer_churn.xlsx     # Original Excel file
â”‚   â””â”€â”€ churn.duckdb                  # DuckDB database
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ convert_excel_to_csv.py       # Excel â†’ CSV converter
â”‚   â””â”€â”€ train_model.py                # ML training script
â”‚
â”œâ”€â”€ ğŸ“‚ models/                        # Trained ML Models
â”‚   â”œâ”€â”€ logistic_regression_model.pkl
â”‚   â”œâ”€â”€ random_forest_model.pkl
â”‚   â”œâ”€â”€ xgboost_model.pkl
â”‚   â”œâ”€â”€ scaler.pkl                    # Feature scaler
â”‚   â”œâ”€â”€ encoded_columns.pkl           # Column names after encoding
â”‚   â”œâ”€â”€ input_template.pkl            # Template for predictions
â”‚   â””â”€â”€ feature_names.txt             # Feature list
â”‚
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â””â”€â”€ streamlit_app.py              # Prediction dashboard
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚   â””â”€â”€ 01_explore_duckdb.ipynb       # Data exploration
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                      # This file
â”œâ”€â”€ ğŸ“„ DEMO_PRESENTATION.md           # Presentation slides
â””â”€â”€ ğŸ“„ .gitignore

```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- Git

### Installation

```bash
# 1. Clone repository
git clone <your-repo-url>
cd customer-churn-dbt

# 2. Install dependencies
pip install -r requirements.txt

# 3. Convert Excel to CSV
python src/convert_excel_to_csv.py

# 4. Run DBT pipeline
cd dbt_project
dbt seed          # Load data into DuckDB
dbt run           # Run transformations
dbt test          # Run data quality tests

# 5. Train ML models
cd ..
python src/train_model.py

# 6. Launch dashboard
streamlit run app/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
```

---

## ğŸ”„ DBT Pipeline Details

### Layer 1: Staging

**File:** `models/staging/stg_customers.sql`

**Purpose:** Clean and standardize raw data

```sql
SELECT 
    "customerID" as customer_id,
    "Gender" as gender,
    "Senior Citizen" as is_senior_citizen,
    "Tenure Months" as tenure_months,
    ...
FROM {{ ref('Telco_customer_churn') }}
```

**Output:** View with clean column names

---

### Layer 2: Intermediate

**File:** `models/intermediate/int_customer_features.sql`

**Purpose:** Feature engineering and encoding

```sql
SELECT
    customer_id,
    -- Encode categorical variables
    CASE WHEN gender = 'Male' THEN 1 ELSE 0 END as gender_encoded,
    CASE 
        WHEN contract_type = 'Two year' THEN 2
        WHEN contract_type = 'One year' THEN 1
        ELSE 0 
    END as contract_encoded,
    ...
FROM {{ ref('stg_customers') }}
```

**Output:** View with encoded features

---

### Layer 3: Marts

**File:** `models/marts/fct_customer_churn.sql`

**Purpose:** Final production dataset

```sql
{{ config(materialized='table') }}

SELECT * 
FROM {{ ref('int_customer_features') }}
WHERE customer_id IS NOT NULL
```

**Output:** Physical table ready for ML (7,043 rows Ã— 28 features)

---

## ğŸ§ª Data Quality Testing

DBT automatically tests your data:

```yaml
# models/marts/schema.yml
models:
  - name: fct_customer_churn
    columns:
      - name: customer_id
        tests:
          - unique          # No duplicate IDs
          - not_null        # No missing IDs
      
      - name: Churn Label
        tests:
          - not_null        # No missing labels
          - accepted_values:
              values: [0, 1] # Only 0 or 1
```

**Run tests:**
```bash
cd dbt_project
dbt test
```

---

## ğŸ¤– Machine Learning

### Models Trained

1. **Logistic Regression** (Baseline)
2. **Random Forest** (Best F1-Score)
3. **XGBoost** (Best ROC-AUC)

### Pipeline Steps

```python
# 1. Load from DuckDB
conn = duckdb.connect('data/churn.duckdb')
df = conn.execute("SELECT * FROM marts.fct_customer_churn").df()

# 2. Handle class imbalance
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X_train, y_train)

# 3. Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_balanced)

# 4. Train models
model.fit(X_scaled, y_balanced)

# 5. Evaluate
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)
```

### Results

| Model | Accuracy | F1-Score | ROC-AUC |
|-------|----------|----------|---------|
| Logistic Regression | 0.72 | 0.44 | 0.70 |
| **Random Forest** | **0.75** | **0.62** | **0.83** |
| XGBoost | 0.77 | 0.60 | 0.84 |

**Best Model:** Random Forest (F1=0.62, ROC-AUC=0.83)

---

## ğŸ“Š Key Features (Top 10)

1. **Tenure Months** (12.3%) - How long customer has been with company
2. **Monthly Charges** (10.1%) - Monthly bill amount
3. **Contract Type** (8.7%) - Month-to-month vs long-term
4. **Total Charges** (7.5%) - Total amount paid
5. **Internet Service** (6.8%) - DSL vs Fiber vs No internet
6. **Tech Support** (5.9%) - Has tech support or not
7. **Payment Method** (5.2%) - How customer pays
8. **Online Security** (4.8%) - Has online security addon
9. **Paperless Billing** (4.3%) - Paperless or paper bills
10. **CLTV** (3.9%) - Customer Lifetime Value

---

## ğŸ’¼ Business Impact

### Retention Strategy

| Risk Level | Churn Probability | Actions | Budget | Timeline |
|------------|-------------------|---------|--------|----------|
| ğŸš¨ High | > 70% | â€¢ 20-30% discount<br>â€¢ Dedicated account manager<br>â€¢ Emergency intervention | 30% ACV | 48 hours |
| âš ï¸ Medium | 40-70% | â€¢ 10-15% retention offer<br>â€¢ Satisfaction survey<br>â€¢ Feature training | 15% ACV | 1-2 weeks |
| âœ… Low | < 40% | â€¢ Loyalty rewards<br>â€¢ Upsell opportunities<br>â€¢ Referral program | 5-10% ACV | Ongoing |

### ROI Projection

**Assumptions:**
- 1,000 customers
- $10,000 average annual revenue per customer
- 20% current churn rate
- 30% reduction in churn with intervention

**Investment:** $270,000/year  
**Retained Revenue:** $1,500,000/year  
**ROI:** **456%** ğŸ‰

---

## ğŸ› ï¸ Common Commands

### DBT Commands

```bash
cd dbt_project

# Run all models
dbt run

# Run specific model
dbt run --select stg_customers

# Run model and all downstream models
dbt run --select stg_customers+

# Test data quality
dbt test

# Generate documentation
dbt docs generate
dbt docs serve

# Clean build artifacts
dbt clean
```

### Python Commands

```bash
# Convert Excel to CSV
python src/convert_excel_to_csv.py

# Train models
python src/train_model.py

# Launch dashboard
streamlit run app/streamlit_app.py
```

---

## ğŸ“š Learning Resources

### DBT
- [DBT Learn (Free Course)](https://courses.getdbt.com/)
- [DBT Documentation](https://docs.getdbt.com/)
- [DBT Best Practices](https://docs.getdbt.com/guides/best-practices)

### DuckDB
- [DuckDB Documentation](https://duckdb.org/docs/)
- [DuckDB Python API](https://duckdb.org/docs/api/python/overview)

### Machine Learning
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Handling Imbalanced Data](https://imbalanced-learn.org/)

---

## ğŸ› Troubleshooting

### DuckDB Installation Issues

```bash
pip install --upgrade pip
pip install duckdb
```

### DBT Seed Errors

If you get errors with `Total Charges`:
```bash
# Re-run the conversion script (it handles missing values)
python src/convert_excel_to_csv.py
cd dbt_project
dbt seed --full-refresh
```

### Model Training Errors

```bash
# Make sure DBT pipeline ran successfully
cd dbt_project
dbt run

# Check data exists in DuckDB
duckdb ../data/churn.duckdb
> SELECT COUNT(*) FROM marts.fct_customer_churn;
> .quit

# Then train
cd ..
python src/train_model.py
```

---

## ğŸ¯ Key Takeaways

### What Makes This Project Different?

1. **SQL-First Approach**
   - All transformations in SQL (not Python)
   - Easy to understand and modify
   - Version controlled in Git

2. **Automated Testing**
   - Data quality tests run automatically
   - Catch issues before they reach ML models

3. **Reproducible**
   - `dbt run` = Entire pipeline
   - No manual steps
   - Same results every time

4. **Production-Ready**
   - Documented code
   - Tested data
   - Scalable architecture

---

## ğŸ“ˆ Future Enhancements

- [ ] Deploy dashboard to cloud (Streamlit Cloud)
- [ ] Add CI/CD pipeline (GitHub Actions)
- [ ] Implement incremental models in DBT
- [ ] Create REST API for predictions
- [ ] Add model monitoring and drift detection
- [ ] Integrate with production database (Snowflake/BigQuery)
- [ ] Build customer segmentation model
- [ ] Add A/B testing framework

---

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file for details

---

## ğŸ™ Acknowledgments

- **Dataset:** Kaggle Telco Customer Churn Dataset
- **Tools:** DBT, DuckDB, Scikit-learn, XGBoost, Streamlit
- **Inspiration:** Modern Data Stack principles

---

## ğŸ“ Contact

- **GitHub:** [Your GitHub Profile]
- **LinkedIn:** [Your LinkedIn]
- **Email:** [Your Email]

---

**â­ If you found this project helpful, please give it a star!**

---

*Built with â¤ï¸ using DBT, DuckDB, and Python*